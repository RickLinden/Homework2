---
title: "Session 4: Homework 2"
author: "Your name goes here"
date: "`r Sys.Date()`"
output:
  html_document:
    theme: flatly
    highlight: zenburn
    number_sections: yes
    toc: yes
    toc_float: yes
    code_folding: show
---


```{r, setup, include=FALSE}
knitr::opts_chunk$set(
  message = FALSE, 
  warning = FALSE, 
  tidy=FALSE,     # display code as typed
  size="small")   # slightly smaller font for code
options(digits = 3)

# default figure size
knitr::opts_chunk$set(
  fig.width=6.75, 
  fig.height=6.75,
  fig.align = "center"
)
```


```{r load-libraries, include=FALSE}
library(tidyverse)  # Load ggplot2, dplyr, and all the other tidyverse packages
library(mosaic)
library(ggthemes)
library(lubridate)
library(here)
library(skimr)
library(janitor)
library(httr)
library(readxl)
library(vroom)
```



# Climate change and temperature anomalies 


```{r weather_data, cache=TRUE}

weather <- 
  read_csv("https://data.giss.nasa.gov/gistemp/tabledata_v3/NH.Ts+dSST.csv", 
           skip = 1, 
           na = "***")

```

```{r}
glimpse(weather)
```


```{r tidyweather}
weather_select <- weather %>% 
  select(Year:Dec)

tidyweather <- weather_select %>% 
  pivot_longer(
    names_to = "Month",
    values_to = "delta",
    cols = -Year
  )
tidyweather

```

## Plotting Information

```{r scatter_plot}

tidyweather <- tidyweather %>%
  mutate(date = ymd(paste(as.character(Year), Month, "1")),
         month = month(date, label=TRUE),
         year = year(date))

tidyweather_plot <- ggplot(tidyweather, aes(x=date, y = delta))+
  geom_point()+
  geom_smooth(color="red") +
  theme_bw() +
  labs (
    title = "Weather Anomalies"
  )

tidyweather_plot

```


```{r facet_wrap, echo=FALSE}

tidyweather_plot +
  facet_wrap(~month)

```

```{r intervals, eval=FALSE}

comparison <- tidyweather %>% 
  filter(Year>= 1881) %>%     #remove years prior to 1881
  #create new variable 'interval', and assign values based on criteria below:
  mutate(interval = case_when(
    Year %in% c(1881:1920) ~ "1881-1920",
    Year %in% c(1921:1950) ~ "1921-1950",
    Year %in% c(1951:1980) ~ "1951-1980",
    Year %in% c(1981:2010) ~ "1981-2010",
    TRUE ~ "2011-present"
  ))

```


```{r density_plot, eval=FALSE}

ggplot(comparison, aes(x=delta, fill=interval))+
  geom_density(alpha=0.2) +   #density plot with tranparency set to 20%
  theme_bw() +                #theme
  labs (
    title = "Density Plot for Monthly Temperature Anomalies",
    y     = "Density"         #changing y-axis label to sentence case
  )

```

So far, we have been working with monthly anomalies. However, we might be interested in average annual anomalies. We can do this by using `group_by()` and `summarise()`, followed by a scatter plot to display the result. 

```{r averaging, eval=FALSE}

#creating yearly averages
average_annual_anomaly <- tidyweather %>% 
  group_by(Year) %>%   #grouping data by Year
  
  # creating summaries for mean delta 
  summarise(annual_average_delta = mean(delta, na.rm=TRUE)) %>% 
  # use `na.rm=TRUE` to eliminate NA (not available) values
  filter(na.rm = TRUE)

#plotting the data:
ggplot(average_annual_anomaly, aes(x=Year, y= annual_average_delta))+
  geom_point()+
  
  #Fit the best fit line, using LOESS method
  geom_line(method = loess) +
  
  #change to theme_bw() to have white background + black frame around plot
  theme_bw() +
  labs (
    title = "Average Yearly Anomaly",
    y     = "Average Annual Delta"
  )                         


```


## Confidence Interval for `delta`

[NASA points out on their website](https://earthobservatory.nasa.gov/world-of-change/decadaltemp.php) that 

> A one-degree global change is significant because it takes a vast amount of heat to warm all the oceans, atmosphere, and land by that much. In the past, a one- to two-degree drop was all it took to plunge the Earth into the Little Ice Age.

```{r, calculate_CI_using_formula, eval=FALSE}

library(infer)

average_annual_anomaly

comparison_filter <- drop_na(average_annual_anomaly) %>%
  # choose the interval 2011-present
  filter(Year > 2011)
  # what dplyr verb will you use? 
  
  # calculate summary statistics for temperature deviation (delta) 
comparison_formula_ci <- comparison_filter %>% 
  
  summarize(mean = mean(annual_average_delta), sd = sd(annual_average_delta), n = n()) %>%

  # calculate mean, SD, count, SE, lower/upper 95% CI
  mutate(se = sd/sqrt(n)) %>%
  mutate(lower_95 = mean - 2*se, upper_95 = mean + 2*se)

# print out formula_CI
comparison_formula_ci
```


```{r, calculate_CI_using_bootstrap}

# use the infer package to construct a 95% CI for delta
bootstrap_comparison <- comparison_filter %>% 
specify(response = annual_average_delta) %>% 
generate(reps = 1000) %>% 
calculate(stat = "mean")

comparison_ci <-bootstrap_comparison %>% 
  get_confidence_interval(level = 0.95, type = "percentile")

comparison_ci

distribution <- bootstrap_comparison %>% 
  visualise() + shade_confidence_interval(endpoints = comparison_ci )

distribution

  
```

> What is the data showing us? Please type your answer after (and outside!) this blockquote. You have to explain what you have done, and the interpretation of the result. One paragraph max, please!

We are 95% confident that the average annual temperature deviation since 2011 is between 0.886 and 1.13. 

# General Social Survey (GSS)

The [General Social Survey (GSS)](http://www.gss.norc.org/) gathers data on American society in order to monitor and explain trends in attitudes, behaviours, and attributes. Many trends have been tracked for decades, so one can see the evolution of attitudes, etc in American Society.



```{r, read_gss_data, cache=TRUE}
gss <- read_csv(here::here("data", "smallgss2016.csv"), 
                na = c("", "Don't know",
                       "No answer", "Not applicable"))
glimpse(gss)
```

## Instagram and Snapchat, by sex

Can we estimate the *population* proportion of Snapchat or Instagram users in 2016?

```{r}
gss_snap_insta <- gss %>%
  # create the new variable for snap_insta
  mutate(snap_insta = case_when(
    snapchat == "Yes" | instagrm == "Yes" ~ "Yes", 
    snapchat == "No" & instagrm == "No" ~ "No",
    TRUE ~ NA_character_)) 
  
# calculate the proportion
gss_prop <- gss_snap_insta %>%
  filter(!is.na(snap_insta)) %>% 
  group_by(snap_insta) %>% 
  summarise(count = n()) %>% 
  mutate(prop = count/sum(count)) 

gss_prop

# percentile_ci
snap_insta_formula_ci <- gss_snap_insta %>%
  group_by(sex) %>%
  # using the CI formula m +- z * se
  summarize(p_hat = prop(snap_insta == "Yes"),
            count = n(),
            se = sqrt((p_hat * (1 - p_hat)) / count),
            t_critical = qt(0.975, count-1),
            margin_of_error = t_critical*se,
            lower_95 = p_hat - margin_of_error,
            upper_95 = p_hat + margin_of_error)

snap_insta_formula_ci

# additional plot for practice -> check the way that he did it in a previous example ?
histogram_si <- ggplot(snap_insta_formula_ci,  
       aes(x = sex,
           y = p_hat,
           colour = sex)) +
   geom_errorbar(aes(ymin = lower_95,
                     ymax = upper_95),
                     width = 0.05,
                     size  = 0.5) +
   geom_point(shape = 15,
              size  = 4) +
   scale_y_continuous(limits=c(0,1)) +
   theme_bw() +
   theme(axis.title   = element_text(face  = "bold")) +
   labs(x = "Gender",
     y = "",
   title = "Women are more likely than men to use Snapchat or Instagram in 2016") +
   theme(legend.position = 'none') + 
   NULL

histogram_si

```


## Twitter, by education level

Can we estimate the *population* proportion of Twitter users by education level in 2016?. 

```{r}
# turn degree from a character variable into a factor variable 
gss_factor <- gss %>%
  mutate(degree = factor(degree, order = TRUE, levels = c("Lt high school", "High school", "Junior college", "Bachelor", "Graduate")))

# create the new variable bachelor_graduate and assign values based on criteria below:
gss_degree <- gss_factor %>%
  mutate(bachelor_graduate = case_when(
    degree %in% c("Bachelor","Master") ~ "Yes", 
    degree %in% c("Lt high school", "High school", "Junior college") ~ "No",
    TRUE ~ "NA"
    )) 

# filter out NAs and filter for bachelor and graduates
gss_bachelor <- gss_degree %>%
  filter(bachelor_graduate == "Yes")


# calculate the proportion of `bachelor_graduate` using twitter -> check error -> actually use " formula for difference in proportion"
gss_twitter_prop <- gss_bachelor %>%
  group_by(twitter) %>%
  filter(twitter != "NA") %>% 
  summarize(n = n()) %>%
  mutate(prop_t = n/sum(n))
  

gss_twitter_prop

# construct CI for `bachelor_graduate` (not) using twitter (but with the NAs in the 100 % )
twitter_formula_ci <- gss_twitter_prop %>%
  filter(twitter != "NA") %>%
   # use the CI formula m +- z * se
  summarize(twitter = twitter,
            p_hat_t = prop_t,
            # se for difference in proportions is sqrt(((p(1-p))/n) + ((p(1-p))/n))
            se_t = sqrt((p_hat_t *(1-p_hat_t))/n),
            count_t = n,
            t_critical_t = qt(0.975, count_t-1),
            margin_of_error_t = t_critical_t*se_t,
            lower_95_t = p_hat_t - margin_of_error_t,
            upper_95_t = p_hat_t + margin_of_error_t)

twitter_formula_ci
  
# check if Confidence Intervals overlap
histogram_t <- ggplot(twitter_formula_ci,  
       aes(x = twitter,
           y = p_hat_t,
           colour = twitter)) +
   geom_errorbar(aes(ymin = lower_95_t,
                     ymax = upper_95_t),
                     width = 0.05,
                     size  = 0.5) +
   geom_point(shape = 15,
              size  = 4) +
   scale_y_continuous(limits=c(0,1)) +
   theme_bw() +
   theme(axis.title   = element_text(face  = "bold")) +
   labs(x = "Twitter usage",
        y = "",
   title = "Twitter usage among people with a bachelor or graduate degree in 2016") +
   theme(legend.position = 'none') + 
   NULL

histogram_t

```


## Email usage

Can we estimate the *population* parameter on time spent on email weekly?

1. Create a new variable called `email` that combines `emailhr` and `emailmin` to reports the number of minutes the respondents spend on email weekly.

2. Visualise the distribution of this new variable. Find the mean and the median number of minutes respondents spend on email weekly. Is the mean or the median a better measure of the typical among of time Americans spend on email weekly? Why?

Answer: The median is a better measure than the mean for the typical amount of time Americans spend on email weekly since the mean is more sensitive to outliers than the median.

3. Using the `infer` package, calculate a 95% bootstrap confidence interval for the mean amount of time Americans spend on email weekly. Interpret this interval in context of the data, reporting its endpoints in “humanized” units (e.g. instead of 108 minutes, report 1 hr and 8 minutes). If you get a result that seems a bit odd, discuss why you think this might be the case.

Answer: The confidence interval and the mean are quite high because, as mentioned above, the mean is very sensitive to the  outlier in the data set. Thus, the values are higher than one might probably expect.

4. Would you expect a 99% confidence interval to be wider or narrower than the interval you calculated above? Explain your reasoning.

Answer: A 99% confidence interval would be wider than a 95% confidence interval since you have to be more confident that the true value (in the case email minutes) falls within the interval we will need to allow more potential values within the interval. The confidence level most commonly adopted is 95%.

```{r}
# create new variable email
gss_email <- gss %>% 
  filter(emailmin != "NA", emailhr != "NA") %>%
  mutate(emailmin = as.numeric(emailmin), emailhr = as.numeric(emailhr)) %>%
  mutate(email = emailhr*60 + emailmin)

# visualise the distribution
plot_email <- ggplot(gss_email, 
  aes(x = email)) + 
  geom_density() + 
  geom_vline(aes(xintercept=mean(email, na.rm = T)),   # Ignore NA values for mean
               color="red", linetype="dashed", size=1) +
  geom_vline(aes(xintercept=median(email, na.rm = T)),   # Ignore NA values for mean
               color="blue", linetype="dashed", size=1) +
  annotate("text", x = 800, y = 0.0025, label = "mean", color="red") +
  annotate("text", x = 500, y = 0.00261, label = "median", color="blue") +
  theme_bw()+
  theme(legend.position = "none") +
  labs(
    title = "Right-skewed distribution of time spent on emails by US adults",
    x = "Minutes spent on emails per week",
    y = " ") +

  NULL

plot_email

# calculate a 95% bootstrap confidence interval for the mean
boot_email <- gss_email %>% 
  specify(response = email) %>% 
  generate(reps = 1000, type = "bootstrap") %>% 
  calculate(stat = "mean") 

  
boot_email

# 95% CI and humanize units
percentile_ci <- boot_email %>%
  get_confidence_interval(level = 0.95, type = "percentile") %>%
  mutate(lower_ci_hum = lower_ci %/% 60 + (lower_ci %% 60)/100, upper_ci_hum = upper_ci %/% 60 + (upper_ci %% 60)/100)

percentile_ci
visualise(boot_email)

```

# Trump's Approval Margins

```{r, cache=TRUE}
# Import approval polls data
approval_polllist <- read_csv(here::here('data', 'approval_polllist.csv'))

# or directly off fivethirtyeight website
# approval_polllist <- read_csv('https://projects.fivethirtyeight.com/trump-approval-data/approval_polllist.csv') 

glimpse(approval_polllist)

# Use `lubridate` to fix dates, as they are given as characters.
```

## Create a plot

```{r trump_margins, echo=FALSE, out.width="100%"}

knitr::include_graphics(here::here("images", "trump_approval_margin.png"), error = FALSE)

```

```{r trump_margins, fig.width=9, fig.height=6}
net_approval_plot <- approval_polllist %>%
  filter(subgroup == "Voters") %>%
  mutate(date = mdy(enddate),
         year = year(date),
         week = week(date
        )) %>%
  group_by(year, week) %>%
  summarise(avg_net_approval = mean(adjusted_approve - adjusted_disapprove),
            sd_net_approval = sd(adjusted_approve - adjusted_disapprove),
            count = n(),
            #get t-critical value with (n-1) degrees of freedom
            t_critical = qt(0.975, count - 1),
            se_net_approval = sd_net_approval / sqrt(count),
            margin_of_error = t_critical * se_net_approval,
            net_approval_low = avg_net_approval - margin_of_error,
            net_approval_high = avg_net_approval + margin_of_error) %>%
  ggplot(aes(week, avg_net_approval)) +
  geom_ribbon(aes(
    ymin = net_approval_low, 
    ymax = net_approval_high), 
    alpha = 0.4
    ) +
  geom_line() +
  geom_point() +
  geom_hline(aes(yintercept = 0), color = "orange") +
  labs(
    title = "Estimating Net Approval (approve - disapprove) for Donald Trump",
    subtitle = "Weekly average of all polls",
    x = "week of the year",
    y = "Average Net Apporval (%)"
  ) +
  facet_wrap(~year)+
  coord_cartesian(
    xlim = c(0, 52), 
    ylim = c(-20, 10
  )) +
  theme_bw()

net_approval_plot
```

## Compare Confidence Intervals

Compare the confidence intervals for `week 15` (6-12 April 2020) and `week 34` (17-23 August 2020). Can you explain what's going on? One paragraph would be enough.

>Answer

- The confidence interval for 'week 15' is [-9.93, -6.197], and for 'week 34' it is a wider [-12.26, -7.553], indicating that the deviation of Trump's polls became larger from April to August. This may be related to the mixed reviews he received in the handling of domestic issues, such as COVID-19 and Black Lives Matter.

```{r, get_data, cache=TRUE}

# load gapminder HIV data
hiv <- read_csv(here::here("data","adults_with_hiv_percent_age_15_49.csv"))
life_expectancy <- read_csv(here::here("data","life_expectancy_years.csv"))

# get World bank data using wbstats
indicators <- c("SP.DYN.TFRT.IN","SE.PRM.NENR", "SH.DYN.MORT", "NY.GDP.PCAP.KD")


library(wbstats)

worldbank_data <- wb_data(country="countries_only", #countries only- no aggregates like Latin America, Europe, etc.
                          indicator = indicators, 
                          start_date = 1960, 
                          end_date = 2016)

# get a dataframe of information regarding countries, indicators, sources, regions, indicator topics, lending types, income levels,  from the World Bank API 
countries <-  wbstats::wb_cachelist$countries

```

You have to join the 3 dataframes (life_expectancy, worldbank_data, and HIV) into one. You may need to tidy your data first and then perform [join operations](http://r4ds.had.co.nz/relational-data.html). Think about what type makes the most sense **and explain why you chose it**.

```{r}
#pivot longer the 2nd to 34th columns of estumated numbers of people living with HIV into one column
hiv_long <- hiv %>%
  pivot_longer(cols=2:34, 
               names_to="year", 
               values_to = "estimated_hiv")

#pivot longer the 2nd to 302nd columns of life expectancy in countries to one column
life_expectancy_long <- life_expectancy %>% 
  pivot_longer(cols=2:302, 
               names_to="year", 
               values_to = "life_expec")

#rename columns in worldban_data
names(worldbank_data)[5] <- "gdpPercap"
names(worldbank_data)[6] <- "female_fertility"
names(worldbank_data)[7] <- "elementary_enrolment"
names(worldbank_data)[8] <- "mortality_rate"
worldbank_data$date <- as.character(worldbank_data$date)

#join life expectancy and expected numbers of hiv patient into lifeExp_hiv
lifeExp_hiv <- left_join(hiv_long, life_expectancy_long, 
                         by = c("year", "country"))

#join worldbank data and lifeExp_hiv into lifeExp_hiv_wb
lifeExp_hiv_wb <- left_join(lifeExp_hiv, worldbank_data, 
                            by = c("year"="date","country"))

#join country data into a complete dataframe
complete_data <- full_join(lifeExp_hiv_wb,countries,
                           by = "country")
complete_data


```

Answer:

We chose to use left_join to join the three dataframes, in the order of HIV, life expectancy, and worldbank data, namely from the dataframe with fewest observations to the one with the most observations; in this way, we ensure that in the final joint dataframe each observation has values from all three priginal dataframe.

1. What is the relationship between HIV prevalence and life expectancy? Generate a scatterplot with a smoothing line to report your results. You may find faceting useful

```{r}
complete_data_filtered <- complete_data %>% drop_na()
ggplot(complete_data_filtered) +
geom_point(aes(
  x=estimated_hiv, 
  y=life_expec,
  colour = region
)) +
geom_smooth(aes(x=estimated_hiv, y=life_expec), se = FALSE)+
labs(title = "HIV prevalence and life expectancy",
    x = "Estimated HIV Patients",
    y = "Life Expectancy"
  )+
NULL

ggplot(complete_data_filtered) +
geom_point(aes(
  x=estimated_hiv, 
  y=life_expec,
)) +
geom_smooth(aes(x=estimated_hiv, y=life_expec), se = FALSE)+
  labs(
    x = "Estimated HIV Patients",
    y = "Life Expectancy"
  )+
facet_wrap(~region, scales = "free")+
NULL

```

Answer:
In general, HIV prevelance seems to have a negative relationship with life expectancy. If we look at the relationship by region, the negative correlation is most obvious in Latin America and Carribean and Sub-Saharan Africa, while in the other 4 regions HIV prevelance do not have a clear linear relationship with life expectancy.


2. What is the relationship between fertility rate and GDP per capita? Generate a scatterplot with a smoothing line to report your results. You may find facetting by region useful

```{r}
ggplot(complete_data_filtered) +
geom_point(aes(
  x = female_fertility, 
  y = gdpPercap
)) +
geom_smooth(aes(
  x = female_fertility, 
  y = gdpPercap)) +
facet_wrap(~region, scales = "free") +
labs(
    x = "Fertility Rate",
    y = "GDP per Capita"
  )+
NULL
```
In general, fertality is positively related to GDP per capita. This positive relationship is most obvious in Latin America and Carribean and Middle East & North Africa.


3. Which regions have the most observations with missing HIV data? Generate a bar chart (`geom_col()`), in descending order.

```{r}
#count missing value in each region
missing_HIV_regions <- complete_data %>% 
  group_by(region) %>% 
  summarise(missing_HIV_data = count(is.na(estimated_hiv))) %>% 
  filter(region != "NA")

#create a bar chart
missing_HIV_regions %>% 
ggplot(aes(
  x = reorder(region, missing_HIV_data),
  y = missing_HIV_data
)) +
  geom_col() +
  coord_flip() +
    labs(
    x = "Region",
    y = "Missing HIV data",
    title = "Miss HIV Data per Region"
  )+
  NULL
  
```
Answer: 
Sub-Saharan Africa has the most missing HIV data. Europe & Central Asia has the second most missing HIV data. 


4. How has mortality rate for under 5 changed by region? In each region, find the top 5 countries that have seen the greatest improvement, as well as those 5 countries where mortality rates have had the least improvement or even deterioration.

```{r}
#create a dataframe with country, mortality rate, year, and region selected
mortality <- complete_data %>%
  select(1,2,10,18)%>%
  drop_na() 

#create scatter plot, facet by region to show changes in mortality rate
ggplot(mortality) +
geom_point(aes(
  x=year, 
  y=mortality_rate,
  color=country
)) +
geom_smooth(aes(x=year, y=mortality_rate), se = FALSE)+
facet_wrap(~region, scales = "free") +
theme(legend.position = "none")+
NULL

#create variable dealta to represent changes in mortality rate
delta <- mortality %>%
  group_by(region) %>% 
  filter(year == min(year) | year == max(year))

#pivot wider the table to separate years
pivot_wider_delta <- delta %>% 
  pivot_wider(names_from = year,
              values_from = mortality_rate) %>% 
  mutate(delta_mort_rate = `2011` - `1979`)

#find out top 5 countries with largest increase in mortality rate
top_5_mort_change <- pivot_wider_delta %>% 
  select(country, delta_mort_rate) %>% 
  arrange(desc(delta_mort_rate)) %>% 
  head(n=5)

#find out top 5 countries with smallest increase / largest decrease in mortality rate
bottom_5_mort_change <- pivot_wider_delta %>% 
  select(country, delta_mort_rate) %>% 
  filter(country != "Serbia") %>% 
  arrange(desc(delta_mort_rate)) %>% 
  tail(n=5)

#create a bar chart in descending order
top_5_plot <- top_5_mort_change %>% 
  ggplot(aes(
    x = reorder(country,-delta_mort_rate),
    y = delta_mort_rate
)) +
  geom_col()+
  labs(x = "Country",
       y = "Change in Mortality Rate",
       title = "Countries with most deterioration in mortality rate"
      )+
NULL

#create a bar chart in descending order
bottom_5_plot <- bottom_5_mort_change %>% 
  ggplot(aes(
    x = reorder(country,delta_mort_rate),
    y = delta_mort_rate
)) +
  geom_col() +
  labs(
    x = "Country",
    y = "Change in Mortality Rate",
    title = "Countries with greatest improvement in mortality rate"
    )+
NULL

#print two bar charts
bottom_5_plot
top_5_plot
  
```

Answer: 
- For most regions, child mortality rate has dropped over the year 1979-2011 and stablize in recent years. One exception is North America, where the countries within the resgion have gone though various fluctuations in mortality rate.

- The five countries with greatest improvement in mortality rate are Oman, Maldives, Bhutan, Bangladesh, Rwanda.
- The five countries with least improvement, in other words, greatest deterioration in mortality rate are Sweden, Netherlands, Finland, Denmark, France.

1. Is there a relationship between primary school enrollment and fertility rate?

```{r}

ggplot(complete_data_filtered) +
  geom_point(aes(
    x=female_fertility, 
    y=elementary_enrolment
    )) +
  geom_smooth(aes(
    x=female_fertility, 
    y=elementary_enrolment), 
    se = FALSE
    ) +
  facet_wrap(~region, 
    scales = "free"
    ) +
  labs(
    x = "Female Fertility Rate",
    y = "Primary School Enrollment",
    title = "Negative relationship between fertility rate and primary school enrollment"
    )+
NULL

```

Answer:
Across all six regions, primary school enrollment is negatively related with fertility rate. It seems intuitive, as when a family has more children, it adds on more financial stress to the parents to put all the children through education, therefore the primary school enrollment rate is lower.



# Challenge 1: CDC COVID-19 Public Use Data

```{r, cache=TRUE}
# URL link to CDC to download data
url <- "https://data.cdc.gov/api/views/vbim-akqf/rows.csv?accessType=DOWNLOAD"

covid_data <- vroom(url)%>%
  clean_names()

```

```{r covid_challenge, echo=FALSE, out.width="100%"}

knitr::include_graphics(here::here("images", "covid_death_rate_comorbidities.png"), error = FALSE)
knitr::include_graphics(here::here("images", "covid_death_rate_icu.png"), error = FALSE)

options(scipen=999)

covid_co_morbidities <- covid_data %>% 
  filter(medcond_yn == "Yes" | medcond_yn == "No", #filter out unknown and missing
         sex == "Male" | sex == "Female", # filter out other, unknown and missing. Other is always zero.
         age_group != "Unknown",
         age_group != "NA"
         ) %>% 
  group_by(medcond_yn, age_group, sex) %>% 
 summarise(death_rate = count(death_yn == "Yes")/n()*100)

covid_ICU <- covid_data %>% 
  filter(icu_yn == "Yes" | icu_yn == "No", #filter out unknown and missing
         sex == "Male" | sex == "Female", # filter out other, unknown and missing. Other is always zero.
         age_group != "Unknown",
         age_group != "NA"
         ) %>% 
  group_by(icu_yn, age_group, sex) %>% 
  summarise(death_rate = count(death_yn == "Yes")/n()*100)

covid_co_morbidities %>% 
  ggplot(aes(
    x = age_group,
    y = death_rate,
    fill=sex,
    label = round(death_rate, digits = 1)
  )) +
  geom_bar(stat="identity",position=position_dodge()) +
  geom_text() +
  coord_flip() +
  facet_grid(vars(medcond_yn), vars(sex))

covid_ICU %>% 
  ggplot(aes(
    x = age_group,
    y = death_rate,
    fill=sex,
    label = round(death_rate, digits = 1)
  )) +
  geom_bar(stat="identity",position=position_dodge()) +
  geom_text() +
  coord_flip() +
  facet_grid(vars(icu_yn), vars(sex))
  
  

```


# Challenge 2: Excess rentals in TfL bike sharing


```{r, get_tfl_data, cache=TRUE}
url <- "https://data.london.gov.uk/download/number-bicycle-hires/ac29363e-e0cb-47cc-a97a-e216d900a6b0/tfl-daily-cycle-hires.xlsx"

# Download TFL data to temporary file
httr::GET(url, write_disk(bike.temp <- tempfile(fileext = ".xlsx")))

# Use read_excel to read it as dataframe
bike0 <- read_excel(bike.temp,
                   sheet = "Data",
                   range = cell_cols("A:B"))

# change dates to get year, month, and week
bike <- bike0 %>% 
  clean_names() %>% 
  rename (bikes_hired = number_of_bicycle_hires) %>% 
  mutate (year = year(day),
          month = lubridate::month(day, label = TRUE),
          week = isoweek(day))
```



```{r tfl_month_year_grid, echo=FALSE, out.width="100%"}
knitr::include_graphics(here::here("images", "tfl_distributions_monthly.png"), error = FALSE)
```

Look at May and Jun and compare 2020 with the previous years. What's happening?

Answer: There are a lot less bike rentals in May and June compared to previous years and there is not a spike as in previous years. This can probably be explained be the Corona virus and lockdown where people used less bikes because more people were staying at home and e.g. working from home.

However, the challenge I want you to work on is to reproduce the following two graphs.

```{r tfl_absolute_monthly_change, echo=FALSE, out.width="100%"}
knitr::include_graphics(here::here("images", "tfl_monthly.png"), error = FALSE)
```

```{r,fig.width=15}

library(lubridate)

bike_1<-bike%>%mutate(date=ymd(day),
              month=month(day,label=TRUE),
              year=year(date))%>%select(bikes_hired:month)

bike_2<-bike_1%>%
  filter(year>=2015)%>%
  group_by(year,month)%>%
  summarise(avg_bikes_hired=mean(bikes_hired))%>%
  group_by(month)%>%
  mutate(all_years_month_avg=mean(avg_bikes_hired))

bike_1
bike_2

ggplot(bike_2, aes(x=month,group=1)) +
  geom_line(aes(y = avg_bikes_hired), color = "darkred") + 
  geom_line(aes(y = all_years_month_avg), color="darkblue")+
facet_wrap(~year) + 
  geom_ribbon(aes(
    x = month,
    ymin = avg_bikes_hired,
    ymax = pmax(avg_bikes_hired,all_years_month_avg)),
              alpha = 0.5,
              fill="red") +
  theme_bw() + 
  geom_ribbon(aes(
    x = month,
    ymin = pmin(avg_bikes_hired,all_years_month_avg),
    ymax = avg_bikes_hired),
    alpha = 0.3,
    fill = "green") +
  ylab("Bikes Hired")+ 
  xlab(" ") +
  labs(
      title = " Monthly Changes in TFL Bike Rental",
      subtitle = " Change from monthly average shown in blue \n and calculated from 2015-2019",
      caption = "Source:TfL, London Data Store") +
  NULL

```

```{r tfl_percent_change, echo=FALSE, out.width="100%"}
knitr::include_graphics(here::here("images", "tfl_weekly.png"), error = FALSE)
```

```{r tfl_percent_change, echo=FALSE, out.width="100%",fig.width=25}

bikes_weekly_all_years<-bike%>%
filter(year >= 2015) %>%
  group_by(week) %>%
  mutate(avg_week_over_all_years = mean(bikes_hired)) %>%
  group_by(year,week) %>%
  mutate(avg_week_single_year = mean(bikes_hired)) %>%
  mutate(pct_change = ((avg_week_single_year-avg_week_over_all_years)/avg_week_over_all_years)*100)

bikes_weekly_all_years


ggplot() +
 geom_ribbon(data = bikes_weekly_all_years[bikes_weekly_all_years$pct_change >= 0, ], 
             aes(
               x = week, 
               ymin = 0, 
               ymax = pct_change),
             alpha=0.5, fill="green") +
  geom_ribbon(
    data = bikes_weekly_all_years[bikes_weekly_all_years$pct_change < 0, ], 
    aes(
      x = week, 
      ymin = pct_change, 
      ymax = 0),
      alpha=0.5, 
      fill="red") +
 geom_line(
   data = bikes_weekly_all_years, 
   aes(
     x = week, 
     y = pct_change
     ))+
  facet_wrap(~year)


```

Should you use the mean or the median to calculate your expected rentals? Why?

Mean, because in this plot we do want to see the extreme values that have influence on the rental of bikes.


# Details

- Who did you collaborate with: Josephine Haag, Mehdi Lembarki Kadiri, Jun Xing, Peijun Xu, Melonica Mohapatra, Rick van der Linden
- Approximately how much time did you spend on this problem set: 30 hours
- What, if anything, gave you the most trouble: The coloring in challenge 2
